# 游깷 n8n + Ollama + Ngrok

Este proyecto permite levantar localmente una instancia de **n8n** conectada con **Ollama** (para ejecutar modelos LLM locales) y exponerla de forma segura a internet usando **Ngrok**, ideal para probar automatizaciones basadas en webhooks con IA generativa.

## 游 Caracter칤sticas

- 游댢 Automatizaci칩n de flujos con [n8n](https://n8n.io/)
- 游뱄 Integraci칩n con modelos locales de LLM mediante [Ollama](https://ollama.com/)
- 游깴 Exposici칩n de tu entorno local con [Ngrok](https://ngrok.com/)
- 丘뙖잺 Variables de entorno autoconfiguradas para facilitar el uso
- 游냡 Soporte para Docker


## 丘뙖잺 Requisitos

- Docker
- [Ngrok](https://ngrok.com/download) (coloca el ejecutable en la ra칤z del proyecto)
- [modelo de ollama instalado en el contenedor de ollama en docker]

## 郊윒잺 C칩mo usar

1. Aseg칰rate de tener `ngrok.exe` en la misma carpeta que el script.
2. Abre PowerShell y ejecuta:

```powershell
./start.ps1
```
Este script:

- Inicia Ngrok en segundo plano

- Obtiene la URL p칰blica

- Actualiza autom치ticamente .env

- Proporciona la URL p칰blica generada por Ngrok, para lanzar n8n como: https://xxxxx.ngrok-free.app

```powershell
./stop.ps1
```
- Detiene ngrok

- Detiene la ejecucion de los contenedores
